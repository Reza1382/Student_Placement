{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary Review:\n",
    "\n",
    "This project examined Student placement data and determined the best placement for a student based on their GPA and test scores.\n",
    "The project utilized various data analysis techniques, including data visualization and statistical analysis, to identify the key factors that influence student placement.\n",
    "The results of the analysis were then used to develop a predictive model that can be used to determine the best placement for a student based on their GPA\n",
    "and test scores. The project demonstrates the application of data analysis and machine learning techniques to real-world problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from scipy import stats \n",
    "from sklearn.model_selection import train_test_split,\n",
    "from sklearn.linear_model import LogisticRegression,\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading the dataset in CSV format\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads a dataset from a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the CSV file containing the dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The loaded dataset\n",
    "    \"\"\"\n",
    "    # Load the dataset from the specified file path\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "\n",
    "df = load_data(\"Placement.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial and expression analyses about the dataset\n",
    "def initial_analysis(data):\n",
    "    \"\"\"\n",
    "    Function to perform initial and exploratory analysis about the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas DataFrame\n",
    "        The dataset to be analyzed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function does the following:\n",
    "        1. Displays the first few rows of the dataset.\n",
    "        2. Displays the summary statistics of the dataset.\n",
    "        3. Displays the information about the dataset.\n",
    "        4. Displays the column names of the dataset.\n",
    "        5. Displays the unique values and their counts for each column.\n",
    "        6. Displays the min minium and maximum values for each column.\n",
    "    \"\"\"\n",
    "    # Display the first few rows of the dataset\n",
    "    print(\"First few rows of the dataset:\")\n",
    "    print(data.head())\n",
    "\n",
    "    # Display the summary statistics of the dataset\n",
    "    print(\"\\n==== Summary statistics of the dataset: ====\")\n",
    "    print(data.describe())\n",
    "\n",
    "    # Display the information about the dataset\n",
    "    print(\"\\n==== Information about the dataset: ====\")\n",
    "    print(data.info())\n",
    "\n",
    "    # Display the column names of the dataset\n",
    "    print(\"\\n==== Column names of the dataset: ====\")\n",
    "    print(data.columns)\n",
    "\n",
    "    # Display the unique values and their counts for each column\n",
    "    print(\"\\n==== Unique values and their counts for each column: ====\")\n",
    "    print(data.nunique())\n",
    "\n",
    "    # Display the min minium and maximum values for each column\n",
    "    print(\"\\n==== Minium values for each column: ====\")\n",
    "    print(data.min())\n",
    "    print(\"\\n==== Maximum values for each column: ====\")\n",
    "    print(data.max())\n",
    "\n",
    "\n",
    "initial_analysis(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage null values in the dataset\n",
    "def manage_null_values(data):\n",
    "    \"\"\"\n",
    "    Check for null values in the dataset, fill them in with the mean for numeric\n",
    "    columns and the mode for categorical columns, and check for null values after\n",
    "    filling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The dataset to manage null values for\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check for null values in the dataset\n",
    "    print(\"Null values in the dataset:\")\n",
    "    print(data.isnull().sum())\n",
    "\n",
    "    # Fill null values in the numeric columns with the mean of the respective columns\n",
    "    numeric_columns = data.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n",
    "\n",
    "    # Fill null values in the categorical columns with the mode of the respective columns\n",
    "    categorical_columns = data.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    data[categorical_columns] = data[categorical_columns].fillna(\n",
    "        data[categorical_columns].mode().iloc[0]\n",
    "    )\n",
    "\n",
    "    # Check for null values after filling\n",
    "    print(\"\\nNull values after filling:\")\n",
    "    print(data.isnull().sum())\n",
    "\n",
    "\n",
    "manage_null_values(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition for plotting the numeric data(bar plot, histogram, box plot, scatter plot, pair plot)\n",
    "def plot_data(data):\n",
    "    \"\"\"\n",
    "    Function to plot the data using different types of plots.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas DataFrame\n",
    "        The dataset to be plotted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function plots the data using different types of plots, including bar plot, histogram, box plot, scatter plot, and pair plot.\n",
    "    All plots are displayed on a single page.\n",
    "    \"\"\"\n",
    "    numeric_columns = [\"CGPA\", \"Internships\", \"Salary (INR LPA)\"]\n",
    "    n_cols = len(numeric_columns)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_cols, 2, figsize=(12, 4 * n_cols))\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    # Plot histograms and box plots\n",
    "    for i, column in enumerate(numeric_columns):\n",
    "        # Histogram\n",
    "        sns.histplot(data[column], kde=True, bins=10, ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f\"Histogram of {column}\")\n",
    "        axes[i, 0].set_xlabel(column)\n",
    "        axes[i, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Box plot\n",
    "        data[column].plot(kind=\"box\", ax=axes[i, 1])\n",
    "        axes[i, 1].set_title(f\"Box plot of {column}\")\n",
    "        axes[i, 1].set_xlabel(column)\n",
    "        axes[i, 1].set_ylabel(\"Values\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create pair plot in a separate figure\n",
    "    plt.figure()\n",
    "    sns.pairplot(data.select_dtypes(include=[\"int64\", \"float64\"]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Data visualization\n",
    "plot_data(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection and removal\n",
    "def detect_outliers(data):\n",
    "    \"\"\"\n",
    "    Detect outliers in the dataset using the Interquartile Range (IQR) method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The dataset to detect outliers in\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The cleaned dataset with outliers removed\n",
    "    \"\"\"\n",
    "    # Create a copy of the data to avoid modifying the original\n",
    "    data_clean = data.copy()\n",
    "\n",
    "    # Process each numeric column\n",
    "    for column in data.select_dtypes(include=[\"int64\", \"float64\"]).columns:\n",
    "        # Calculate quartiles and IQR\n",
    "        Q1 = data[column].quantile(0.25)\n",
    "        Q3 = data[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Calculate outlier bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Find outliers\n",
    "        outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "\n",
    "        # Print outlier information\n",
    "        print(f\"\\nColumn: {column}\")\n",
    "        print(f\"Lower bound: {lower_bound:.2f}\")\n",
    "        print(f\"Upper bound: {upper_bound:.2f}\")\n",
    "        print(f\"Number of outliers: {len(outliers)}\")\n",
    "\n",
    "        if len(outliers) > 0 and len(outliers) < 10:\n",
    "            print(f\"Outlier values: {sorted(outliers[column].tolist())}\")\n",
    "\n",
    "        # Remove outliers from the cleaned dataset\n",
    "        data_clean = data_clean[\n",
    "            (data_clean[column] >= lower_bound) & (data_clean[column] <= upper_bound)\n",
    "        ]\n",
    "\n",
    "    return data_clean\n",
    "\n",
    "\n",
    "# Detect and remove outliers\n",
    "df_clean = detect_outliers(df)\n",
    "print(\"\\nCleaned dataset:\")\n",
    "print(df_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df_clean[\"CGPA_grade\"] = pd.cut(\n",
    "    df_clean[\"CGPA\"],\n",
    "    bins=[0, 3.0, 5.0, 8.0, 10.0],\n",
    "    labels=[\"poor\", \"average\", \"good\", \"excellent\"],\n",
    ")\n",
    "\n",
    "df_clean[\"Salary (INR LPA)_grade\"] = pd.cut(\n",
    "    df_clean[\"Salary (INR LPA)\"],\n",
    "    bins=[0, 5, 10, 15, 20, 25, 30],\n",
    "    labels=[\"Entry Level\", \"Junior\", \"Mid-Level\", \"Senior\", \"Expert\", \"Executive\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dfinition for plotting the categorical data\n",
    "def plot_categorical_data(data):\n",
    "    \"\"\"\n",
    "    Function to plot the categorical data using different types of plots.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas DataFrame\n",
    "        The dataset to be plotted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function plots the categorical data using different types of plots, including bar plot, count plot, and box plot.\n",
    "    All plots are displayed on a single page.\n",
    "    \"\"\"\n",
    "\n",
    "    categorical_columns = [\n",
    "        col\n",
    "        for col in data.columns\n",
    "        if data[col].dtype == \"object\" or data[col].dtype == \"category\"\n",
    "    ]\n",
    "    n_cols = len(categorical_columns)\n",
    "\n",
    "    range_info = {\n",
    "        \"CGPA_grade\": {\n",
    "            \"Poor\": \"0-3.0\",\n",
    "            \"Average\": \"3.0-5.0\",\n",
    "            \"Good\": \"5.0-8.0\",\n",
    "            \"Excellent\": \"8.0-10.0\",\n",
    "        },\n",
    "        \"Salary (INR LPA)_grade\": {\n",
    "            \"Entry Level\": \"0-5 LPA\",\n",
    "            \"Junior\": \"5-10 LPA\",\n",
    "            \"Mid-Level\": \"10-15 LPA\",\n",
    "            \"Senior\": \"15-20 LPA\",\n",
    "            \"Expert\": \"20-25 LPA\",\n",
    "            \"Executive\": \"25-30 LPA\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_cols, 1, figsize=(8, 4 * n_cols))\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    # count plots\n",
    "    for i, column in enumerate(categorical_columns):\n",
    "        # Count plot\n",
    "        sns.countplot(\n",
    "            x=column, hue=column, data=data, ax=axes[i], palette=\"viridis\", legend=False\n",
    "        )\n",
    "        axes[i].set_title(f\"Count plot of {column}\")\n",
    "        axes[i].set_xlabel(column)\n",
    "        axes[i].set_ylabel(\"Count\")\n",
    "        if column in range_info.keys():\n",
    "            legend = axes[i].legend(\n",
    "                range_info[column].values(),\n",
    "                loc=\"upper left\",\n",
    "                title=column,\n",
    "                bbox_to_anchor=(1.05, 1),\n",
    "                fontsize=10,\n",
    "            )\n",
    "            legend.get_title().set_fontsize(12)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "plot_categorical_data(df_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro test for normality in numeric columns\n",
    "def normality_test(data):\n",
    "    \"\"\"\n",
    "    Perform Shapiro test for normality in numeric columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The dataset to perform Shapiro test on\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"=\" * 60\n",
    "        + \"\\n Shapiro test for normality in numeric columns \\n\"\n",
    "        + \"=\" * 60\n",
    "        + \"\\n\"\n",
    "    )\n",
    "    numeric_columns = [\"CGPA\", \"Internships\", \"Salary (INR LPA)\"]\n",
    "    for column in numeric_columns:\n",
    "        print(f\"Shapiro test for {column}:\")\n",
    "        statistic, p_value = stats.shapiro(data[column])\n",
    "        print(f\"Statistic: {statistic:.4f}\")\n",
    "        print(f\"P-value: {p_value:.2e}\")\n",
    "        print(\n",
    "            f\"Normality: {'Normal(p-value > 0.05)' if p_value > 0.05 else 'Not normal(p-value <= 0.05)'}\"\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "normality_test(df_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test checks if your data follows a normal distribution. For all three variables (CGPA, Internships, and Salary), the test indicates non-normality because all p-values are below 0.05.\n",
    "\n",
    " - **CGPA** (Statistic: 0.9968, P-value: 0.0394) is borderline non-normal. The test statistic is close to 1, suggesting it's nearly normal but doesn't quite meet the statistical threshold.\n",
    "\n",
    " - **Internships** (Statistic: 0.8839, P-value: 1.32e-26) shows strong evidence of non-normality with a very low p-value.\n",
    "\n",
    " - **Salary** (Statistic: 0.8858, P-value: 2.06e-26) also demonstrates strong non-normality, likely due to a skewed distribution.\n",
    "\n",
    "Since none of these variables are normally distributed, you should use non-parametric statistical methods instead of those that assume normality (like t-tests or ANOVA).Consider using tests like Mann-Whitney U or Kruskal-Wallis for your analyses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation between CGPA, internships and salary\n",
    "def spearman_correlation(data):\n",
    "    \"\"\"\n",
    "    Calculate Spearman correlation between CGPA, internships and salary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The dataset to calculate Spearman correlation on\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"=\" * 60\n",
    "        + \"\\n Spearman correlation between CGPA, internships and salary \\n\"\n",
    "        + \"=\" * 60\n",
    "        + \"\\n\"\n",
    "    )\n",
    "    correlation = data[[\"CGPA\", \"Internships\", \"Salary (INR LPA)\"]].corr(\n",
    "        method=\"spearman\"\n",
    "    )\n",
    "    print(\"Spearman correlation between CGPA, internships and salary\")\n",
    "    ax = sns.heatmap(correlation, annot=True, cmap=\"coolwarm\")\n",
    "    ax.set_title(\"Spearman correlation heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "    print(correlation)\n",
    "\n",
    "\n",
    "spearman_correlation(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spearman correlation heatmap shows the correlation between CGPA, Internships, and Salary (INR LPA).\n",
    "The Spearman correlation results indicate that:\n",
    "\n",
    "- There is a weak negative correlation between CGPA and Internships. (correlation coefficient: -0.020540)\n",
    "- There is a weak negative correlation between CGPA and Salary (INR LPA). (correlation coefficient: -0.065308)\n",
    "- There is a near-zero correlation between Internships and Salary (INR LPA). (correlation coefficient: -0.001089)\n",
    "\n",
    "Overall, the results suggest that there is no significant correlation between the variables CGPA, Internships, and Salary (INR LPA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann-Whitney U test between 'CGPA', 'Internships' and 'Salary' with 'Placed'\n",
    "df_clean[\"placed_binary\"] = df_clean[\"Placed\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "\n",
    "def mann_whitney_test(data):\n",
    "    \"\"\"\n",
    "    Perform Mann-Whitney U test between 'CGPA', 'Internships' and 'Salary' with 'Placed'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The dataset to perform Mann-Whitney U test on\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"=\" * 60\n",
    "        + \"\\n Mann-Whitney U test between 'CGPA', 'Internships' and 'Salary' with 'Placed' \\n\"\n",
    "        + \"=\" * 60\n",
    "        + \"\\n\"\n",
    "    )\n",
    "    for column in [\"CGPA\", \"Internships\", \"Salary (INR LPA)\"]:\n",
    "        print(f\"Mann-Whitney U test between {column} and Placed:\")\n",
    "        statistic, p_value = stats.mannwhitneyu(data[column], data[\"placed_binary\"])\n",
    "        print(f\"Statistic: {statistic:.4f}\")\n",
    "        print(f\"P-value: {p_value:.2e}\")\n",
    "        print(\n",
    "            f\"Significance: {'Significant(p-value < 0.05)\\n' if p_value < 0.05 else 'Not significant(p-value >= 0.05)\\n'}\"\n",
    "        )\n",
    "\n",
    "\n",
    "mann_whitney_test(df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Mann-Whitney U test indicate a significant relationship between the variables 'CGPA', 'Internships', and 'Salary (INR LPA)' and the variable 'Placed'.\"\n",
    "The extremely low p-values (0.00e+00, 2.56e-105, and 3.18e-80) suggest that these relationships are statistically significant.\"\n",
    "These findings suggest that there is a strong association between these variables and the likelihood of being placed in a job.\"\n",
    "Therefore, it is recommended to further investigate these relationships and explore the potential predictive power of these variables in determining job placement outcomes.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Logistic Regression Model\n",
    "\n",
    "X = df_clean[[\"CGPA\", \"Internships\", \"Salary (INR LPA)\"]]\n",
    "y = df_clean[\"Placed\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "print(\"Classification Report:\\n\", classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Report of Logistic Regression Model Results**\n",
    "\n",
    "Overall Model Performance:\n",
    "- Overall Accuracy: 1.00 (100%)\n",
    "- Average of Precision, Recall, and F1-Score: 1.00 (100%)\n",
    "\n",
    "Confusion Matrix Analysis:\n",
    "- Out of 93 samples in the \"No\" class, all 93 were correctly predicted (False Positives = 0).\n",
    "- Out of 205 samples in the \"Yes\" class, all 205 were correctly predicted (False Negatives = 0).\n",
    "\n",
    "Detailed Performance for Each Class:\n",
    "1. \"No\" Class:\n",
    "   - Precision: 1.00 (100%)\n",
    "   - Recall: 1.00 (100%)\n",
    "   - F1-Score: 1.00 (100%)\n",
    "\n",
    "2. \"Yes\" Class:\n",
    "   - Precision: 1.00 (100%)\n",
    "   - Recall: 1.00 (100%)\n",
    "   - F1-Score: 1.00 (100%)\n",
    "\n",
    "Conclusion:\n",
    "The logistic regression model has demonstrated perfect performance on the test data.\n",
    "All evaluation metrics (precision, recall, and F1-score) for both classes have reached 1.00 (100%), indicating the model's complete ability to distinguish between the two classes.\n",
    "Given the absence of errors in the confusion matrix, it can be concluded that the model is not overfitting on this dataset and maintains a good balance between the two classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier Model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "classification = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "print(\"Classification Report:\\n\", classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Report of Random Forest Classifier Model Results**\n",
    "\n",
    "Overall Model Performance:\n",
    "- Overall Accuracy: 1.00 (100%)\n",
    "- Average of Precision, Recall, and F1-Score: 1.00 (100%)\n",
    "\n",
    "Confusion Matrix Analysis:\n",
    "- Out of 93 samples in the \"No\" class, all 93 were correctly predicted (False Positives = 0).\n",
    "- Out of 205 samples in the \"Yes\" class, all 205 were correctly predicted (False Negatives = 0).\n",
    "\n",
    "Detailed Performance for Each Class:\n",
    "1. \"No\" Class:\n",
    "   - Precision: 1.00 (100%)\n",
    "   - Recall: 1.00 (100%)\n",
    "   - F1-Score: 1.00 (100%)\n",
    "\n",
    "2. \"Yes\" Class:\n",
    "   - Precision: 1.00 (100%)\n",
    "   - Recall: 1.00 (100%)\n",
    "   - F1-Score: 1.00 (100%)\n",
    "\n",
    "Conclusion:\n",
    "The random forest classifier model has demonstrated perfect performance on the test data.\n",
    "All evaluation metrics (precision, recall, and F1-score) for both classes have reached 1.00 (100%), indicating the model's complete ability to distinguish between the two classes.\n",
    "Given the absence of errors in the confusion matrix, it can be concluded that the model is not overfitting on this dataset and maintains a good balance between the two classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation for Logistic Regression and Random Forest\n",
    "\n",
    "# Define logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define 5-fold stratified cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "logreg_scores = cross_val_score(logreg, X, y, cv=skf, scoring=\"accuracy\")\n",
    "\n",
    "# Print out the cross-validation accuracy scores for logistic regression model\n",
    "print(\"Logistic Regression CV Accuracy Scores:\", logreg_scores)\n",
    "\n",
    "# Calculate and print out the mean accuracy for logistic regression model\n",
    "print(\"Mean Accuracy:\", np.mean(logreg_scores))\n",
    "print(\"Standard Deviation:\", np.std(logreg_scores))\n",
    "rf_scores = cross_val_score(rf, X, y, cv=skf, scoring=\"accuracy\")\n",
    "\n",
    "# Print out the cross-validation accuracy scores for random forest model\n",
    "print(\"\\nRandom Forest CV Accuracy Scores:\", rf_scores)\n",
    "print(\"Mean Accuracy:\", np.mean(rf_scores))\n",
    "print(\"Standard Deviation:\", np.std(rf_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short Report: Cross-Validation Results**\n",
    "\n",
    "Summary:\n",
    "\n",
    "A 5-fold stratified cross-validation was performed to compare a Logistic Regression model and a Random Forest classifier. Both models were evaluated based on accuracy.\n",
    "\n",
    "Results:\n",
    "- Logistic Regression: Achieved a perfect mean accuracy of 1.0 with a standard deviation of 0.0.\n",
    "- Random Forest: Also achieved a perfect mean accuracy of 1.0 with a standard deviation of 0.0.\n",
    "\n",
    "Conclusion:\n",
    "- Both models demonstrated flawless and identical performance on this dataset, correctly classifying all samples in every validation fold with no variance in their results.\n",
    "\n",
    "___________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Placement Analysis - Project Summary\n",
    "\n",
    "**Project Overview**\n",
    "\n",
    "This project analyzed student placement data to predict job placement outcomes using CGPA, internship experience, and salary expectations through machine learning techniques.\n",
    "\n",
    "**Data Processing & Analysis**\n",
    "- Data Cleaning: Handled missing values using mean/mode imputation and removed outliers using IQR method\n",
    "- Feature Engineering: Created categorical grades for CGPA and salary levels\n",
    "- Statistical Testing: Applied Shapiro-Wilk tests (confirmed non-normal distributions) and Mann-Whitney U tests (showed significant relationships between features and placement)\n",
    "- Correlation Analysis: Used Spearman correlation revealing weak correlations between variables\n",
    "\n",
    "**Machine Learning Models**\n",
    "- Models Used: Logistic Regression and Random Forest Classifier\n",
    "- Data Split: 70/30 train-test split with stratified sampling\n",
    "- Validation: 5-fold stratified cross-validation\n",
    "\n",
    "**Key Results**\n",
    "- Perfect Performance: Both models achieved 100% accuracy on test data\n",
    "- Cross-Validation: Mean accuracy of 1.0 with zero standard deviation across all folds\n",
    "- Statistical Significance: All features showed significant relationships with placement outcomes (p < 0.001)\n",
    "\n",
    "**Technical Implementation**\n",
    "- Libraries: pandas, numpy, matplotlib, seaborn, scipy, scikit-learn\n",
    "- Code Quality: Modular functions with comprehensive documentation and reproducible results\n",
    "\n",
    "**Conclusions**\n",
    "\n",
    "The project successfully developed highly accurate predictive models for student placement outcomes. The perfect performance suggests strong predictive power of academic performance, internship experience, and salary expectations for job placement success. Results can effectively support career counseling and placement decision-making processes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
